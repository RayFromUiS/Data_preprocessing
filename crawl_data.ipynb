{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ogb.state.ms.us/prodsearch.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "fields = []\n",
    "res = req.get(url)\n",
    "print(res.status_code)\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "fields_option = soup.find(\"select\", id='FieldName').find_all('option')\n",
    "for field_name in fields_option:\n",
    "    field = field_name.text\n",
    "    if field != 'No Preference':\n",
    "        fields.append(field)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## field data to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_data(soup):\n",
    "    '''get table data from passed in soup\n",
    "    input;soup\n",
    "    output:dataframe with columns name\n",
    "    '''\n",
    "    field_column_name = ['Well Info','Report Period','API (10)','OperatorName','Well Name','Oil (bbls)','Water (bbls)','Gas (mcf)'\n",
    "                         ,'Gas Tarns','Field Name','Pool Name','Country Name','Well Type','Well Status']\n",
    "    table_row_id = \"example\"\n",
    "     ## find the table data inside the soup\n",
    "    table_data = soup.find(lambda tag: tag.name=='table' and tag.has_attr('id') and tag['id']==table_row_id)\n",
    "    rows = table_data.findAll(lambda tag: tag.name=='tr'and  tag.has_attr('class'))\n",
    "    df_row = list()\n",
    "    ## scrapy data\n",
    "    for j in range(len(rows)): \n",
    "        ## for each datable row\n",
    "        tds = rows[j].find_all('td')\n",
    "#             print(tds)\n",
    "#             print(j)\n",
    "        df_col = list()\n",
    "        for k in range(len(tds)):\n",
    "#                 print(k)\n",
    "            value = tds[k].text\n",
    "            ## df define is the problem\n",
    "            df_col.append(value)\n",
    "#                 print(k)  \n",
    "#         print(df_col)\n",
    "        df_row.append(df_col)\n",
    "    ## modify df_row to dataframe\n",
    "    df = pd.DataFrame (df_row,columns=field_column_name)\n",
    "    \n",
    "    return df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function drive the explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php'):\n",
    "    ''' function to choose a certain field from option list\n",
    "        and load the production web\n",
    "    '''\n",
    "    table_row_id = \"example\"\n",
    "    \n",
    "    field_data = {}\n",
    "    field_data_each_page = []\n",
    "    ##using soup to get fields list\n",
    "    fields = []\n",
    "    res = req.get(url)\n",
    "    print(res.status_code)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    fields_option = soup.find(\"select\", id='FieldName').find_all('option')\n",
    "    for field_name in fields_option:\n",
    "        field = field_name.text\n",
    "        if field != 'No Preference':\n",
    "            fields.append(field)\n",
    "            \n",
    "#     print(fields[0:5])\n",
    "    ##select field name and submit request\n",
    "    for field in fields[0:40] :\n",
    "#         if field not in field_first_crawl:\n",
    "            ## initiate the driver\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(url)\n",
    "            sleep(3)\n",
    "            select_element = driver.find_element_by_id('FieldName')\n",
    "            select_object = Select(select_element)\n",
    "            select_object.select_by_value(field)\n",
    "            driver.find_element(By.XPATH, '//button[1]').submit()\n",
    "\n",
    "            ## parse new page_source as a soup\n",
    "            first_page_soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            ##get the data from current page\n",
    "            first_page_data = get_page_data(first_page_soup)\n",
    "            field_data_each_page.append(first_page_data)\n",
    "\n",
    "            ##browsing all the left pages\n",
    "            ## get the maximum page number\n",
    "            pages = first_page_soup.find_all('a',attrs = {'class':'paginate_button'})\n",
    "    #         print(pages)\n",
    "            page_list= []\n",
    "            for page in pages:\n",
    "                page_num  = page.text\n",
    "                page_list.append(page_num) \n",
    "    #         print(page_list)\n",
    "            click_next_times = int(page_list[-2])\n",
    "\n",
    "            ##click the 'next_page ' button\n",
    "            for i in range(click_next_times):\n",
    "                driver.find_element(By.CSS_SELECTOR, 'a.next').click()\n",
    "                next_page_source = driver.page_source\n",
    "                next_page_soup = BeautifulSoup(next_page_source,'html.parser')\n",
    "                other_page_data = get_page_data(next_page_soup)\n",
    "                field_data_each_page.append(other_page_data)\n",
    "\n",
    "            ## save each field data to field_data dictionary\n",
    "            field_data[field] = field_data_each_page\n",
    "            driver.close()\n",
    "            print(field,'Field data is saved')\n",
    "        except NoSuchElementException:\n",
    "            print(\"Cannot locate option with value: Doak's Creek\")\n",
    "        \n",
    "    return field_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Abbott Field data is saved\n",
      "Aberdeen Field data is saved\n",
      "Aberdeen E. Field data is saved\n",
      "Aberdeen S. Field data is saved\n",
      "Addie Mae Field data is saved\n",
      "Alcorn Field data is saved\n",
      "ALFRED C. MOORE Field data is saved\n",
      "Alloway Field data is saved\n",
      "Amory S. Field data is saved\n",
      "Anchorage Field data is saved\n",
      "Anna Field data is saved\n",
      "Ansley Field data is saved\n",
      "Apollo E. Field data is saved\n",
      "Apple Grove Field data is saved\n",
      "Ariel N. Field data is saved\n",
      "Arlington Field data is saved\n",
      "Arm Dome Field data is saved\n",
      "Armstrong Field data is saved\n",
      "Armstrong S. Field data is saved\n",
      "Arnot Field data is saved\n",
      "Artonish Field data is saved\n",
      "Artonish E Field data is saved\n",
      "Ashwood Field data is saved\n",
      "Ashwood E. Field data is saved\n",
      "Ashwood S. Field data is saved\n",
      "Athens Field data is saved\n",
      "Auburn Field data is saved\n",
      "Avera Field data is saved\n",
      "Bacon Field data is saved\n",
      "Bakers Creek Field data is saved\n",
      "Bald Hill Field data is saved\n",
      "Balls Branch Field data is saved\n",
      "Barber Creek Field data is saved\n",
      "Barber Creek E. Field data is saved\n",
      "Barnett Field data is saved\n",
      "Barnett Lake Field data is saved\n",
      "Barnett W. Field data is saved\n",
      "Bassfield Field data is saved\n",
      "Batteast Hill Field data is saved\n",
      "Batteast Hill W. Field data is saved\n"
     ]
    }
   ],
   "source": [
    "field_data01_39 = get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php') ## Done\n",
    "#### field_data_40 is not crawled\n",
    "# field_data_41_80 = get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php')  ##Done\n",
    "# field_data_Bolton_Dixons_Bayou = get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php') \n",
    "# field_data_248_258 = get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php')\n",
    "# field_data_258_268 =get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php')\n",
    "# field_data_268_278 =get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php')\n",
    "# field_data_278_288 =get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php')\n",
    "# field_data_288_300 =get_field_page_source(url ='http://www.ogb.state.ms.us/prodsearch.php')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and save the crawling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_crawling_data(crawled_dict):\n",
    "    \n",
    "    ##container for saving the field dataframe\n",
    "    all_fields= []\n",
    "    ## generate df lists for fields\n",
    "    for k in crawled_dict.keys():\n",
    "        field = crawled_dict[k]\n",
    "        each_field_stack = [] \n",
    "        ## store df_row in 'each_field_stack'\n",
    "        for i in range(len( field)):\n",
    "            row = field[i]\n",
    "            df_row = pd.DataFrame(row)\n",
    "            each_field_stack.append(df_row)\n",
    "        ##concate one field rows together\n",
    "        each_field = pd.concat(each_field_stack)\n",
    "        all_fields.append(each_field)\n",
    "    #concate field dataframe together\n",
    "    all_fields_df = pd.concat(all_fields)\n",
    "    return all_fields_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the data\n",
    "field_data01_03_preprocessed = get_df_from_crawling_data(field_data01_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_data01_03_preprocessed.to_csv('field_data01_03_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
