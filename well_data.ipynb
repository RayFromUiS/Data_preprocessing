{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "pd.options.display.max_columns = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path):\n",
    "    \n",
    "    '''get files under certain path\n",
    "\n",
    "    Args:\n",
    "        file path,string\n",
    "\n",
    "    Returns:\n",
    "        file names saved inside list\n",
    "\n",
    "    Raise;\n",
    "        ValueError,file path name\n",
    "\n",
    "    '''\n",
    "    file_paths = [] ## saving file path list\n",
    "    basepath = Path(file_path)\n",
    "    files_in_basepath = basepath.iterdir() #Iterate over the files in this directory \n",
    "    for item in files_in_basepath:\n",
    "        file_paths.append(item) ## append item\n",
    "        \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lease_api(file_paths,lease_index):\n",
    "    \n",
    "    ''' Get lease api pair for unique api number\n",
    "    Args:\n",
    "        lease file path,[]\n",
    "    Return:\n",
    "        lease api dictionary,{lease_number:api_number}\n",
    "    Raise:\n",
    "        KeyError\n",
    "        \n",
    "    '''\n",
    "    lease_api_dfs = []\n",
    "    lease_col_name = 'LEASE_NO' ##lease column name\n",
    "    country_code = 'API_COUNTY_CODE' ## api country code col name\n",
    "    unique_id = 'API_UNIQUE_NO' ## api col\n",
    "    api_col_name ='API_NUMBER' ##mixed api number col\n",
    "    lease_df =  pd.read_csv(file_paths[lease_index],delimiter='}')\n",
    "    ##concate the api number\n",
    "    lease_df[api_col_name] = lease_df[country_code].astype('str') + lease_df[unique_id].astype('str')\n",
    "\n",
    "    ##get the lease only for the case of unique lease\n",
    "    lease_grouped = lease_df.groupby(lease_col_name).groups ## group data\n",
    "    for key,val in lease_grouped.items():    ## iterate the grouped index\n",
    "        if len(val.to_list()) == 1: ## to determine whether is one col\n",
    "            lease_api_df = lease_df.iloc[val][[lease_col_name,api_col_name]]\n",
    "            lease_api_dfs.append(lease_api_df)\n",
    "    return lease_api_dfs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_dataframe = get_lease_api(file_paths,-1)\n",
    "# api_dataframes = pd.concat(api_dataframe)\n",
    "# api_dataframes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(production_index,lease_api_dfs):\n",
    "    '''\n",
    "    get chunk file and preprocess it\n",
    "    \n",
    "    Args:\n",
    "        production file index, interger\n",
    "        lease_api_dfs,[list of dataframe]\n",
    "        \n",
    "    Return:\n",
    "        preprocess dataframe list,[list of dataframe]\n",
    "        \n",
    "    Raise:\n",
    "        KeyError\n",
    "    '''\n",
    "    retrieve_datas = []\n",
    "    lease_col = 'LEASE_NO'\n",
    "    lease_api_index = len(lease_api_dfs)\n",
    "    chunksize = 100000\n",
    "    ## get lease api dataframe\n",
    "    lease_api_df = pd.concat(lease_api_dfs)\n",
    "    lease = lease_api_df[lease_col].values ## get lease series\n",
    "    ## get sliced dataframe from chunk\n",
    "    file_dfs = pd.read_csv(file_paths[production_index],delimiter= '}',low_memory=False\n",
    "                           chunksize=chunksize,encoding = 'unicode_escape')\n",
    "    for chunk in file_dfs:\n",
    "        mask = chunk[lease_col].isin(lease) ##bulid mask if production lease in lease_api pair\n",
    "        if mask.any():\n",
    "            retrieve_data = chunk[mask] ## mask the data\n",
    "            retrieve_data = pd.merge(retrieve_data,lease_api_df,how='left',on=lease_col) ## add api_number to mereged df\n",
    "            \n",
    "            print('yielding production data with merged api number')\n",
    "            yield retrieve_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_monthly(chunk):\n",
    "    '''\n",
    "    retrive data with according to lease_api correlation\n",
    "    \n",
    "    Args:\n",
    "        chunk,dataframe for origianl production csv\n",
    "        lease_pai:dictionary data from function\n",
    "    Returns:\n",
    "        preprocessed dataframe\n",
    "    Raise:\n",
    "        KeyError\n",
    "    '''\n",
    "\n",
    "    standard_cols= ['Well ID','Well name','Month (yyyymm)','Monthly days','Month oil (bbl)','Month water (bbl)',\\\n",
    "                    'Month gas (mmscf)','Month condensate (bbl)','Month water injection (bbl)','Month gas injection (mmscf)',\\\n",
    "                    'Month steam injection (bbl)','Reservoir','Field'] ## standard columns\n",
    "    select_cols = ['OIL_GAS_CODE','LEASE_NO','API_NUMBER','CYCLE_YEAR_MONTH','FIELD_NAME','LEASE_OIL_PROD_VOL',\n",
    "                   'LEASE_GAS_PROD_VOL','LEASE_COND_PROD_VOL'] ## select column to preprocess\n",
    "    \n",
    "    pre_chunk = pd.DataFrame()\n",
    "\n",
    "    pre_chunk = chunk[select_cols] ## slicing the dataframe\n",
    "    \n",
    "    pre_chunk.columns = ['OIL_GAS_CODE','LEASE_NO','Well ID','Month (yyyymm)','Field','Month oil (bbl)',\\\n",
    "                           'Month gas (mmscf)','Month condensate (bbl)'] ## rename the columns\n",
    "    ## add column nonexisted comparing to standard_cols\n",
    "    for col in standard_cols:\n",
    "        if col not in  pre_chunk.columns:\n",
    "              pre_chunk[col] = None\n",
    "    ## unit convert for gas mcf\n",
    "    \n",
    "#     if standard_df['Month gas (mmscf)'].dtype != 'object':\n",
    "    pre_chunk['Month gas (mmscf)'] = pre_chunk['Month gas (mmscf)'].astype('str').\\\n",
    "                                        apply(lambda x:x.strip().replace(',','')).map(float)\n",
    "    pre_chunk['Month gas (mmscf)'] = pre_chunk['Month gas (mmscf)'].div(1000).round(6) \n",
    "#     pre_chunk['Month oil (bbl)'] = pre_chunk['Month oil (bbl)']\\\n",
    "#                                         .apply(lambda x:x.strip().replace(',','')).map(float)\n",
    "#     pre_chunk['Month water (bbl)'] = pre_chunk['Month water (bbl)'] \\\n",
    "#                                         .apply(lambda x:x.strip().replace(',','')).map(float)\n",
    "    ## slcing the df\n",
    "#     pre_chunk = pre_chunk[standard_cols] \n",
    "    \n",
    "    print('yielding preprocess production data')\n",
    "    yield  pre_chunk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engine(database_name):\n",
    "    '''\n",
    "    create connection engine via sqlalchemy\n",
    "    '''\n",
    "    return create_engine(\"mysql://root:python_developer2019@localhost/\"+database_name,encoding='latin1', echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''preprocess data to generate a production dataframe for unique api-lease number\n",
    "    '''\n",
    "    base_dir = 'PDQ_DSV' ## directory to holde all the csv file\n",
    "    lease_index = -1 ##lease file index\n",
    "    production_index = 8 ## production file index\n",
    "    dir_to_save_data = 'well_pro' ## dir to save csv file\n",
    "    wells_df = [] ## well df container   \n",
    "    database_name = 'usa_texas'\n",
    "    well_dynamcis_table = 'well_dynamcis' ## table name under certain database\n",
    "    \n",
    "#     con = create_engine(database_name) ## sqlalchemy connection engine\n",
    "    \n",
    "    file_paths = get_file_name(base_dir) ## read in all the file path\n",
    "    lease_api_dfs = get_lease_api(file_paths,lease_index) ## generate lease api dataframe\n",
    "    retrieve_api_data = preprocess_chunk(production_index,lease_api_dfs) ## preprocess chunk to have the generator\n",
    "    \n",
    "    for chunk in retrieve_api_data: ##iterate the chunk\n",
    "        well_chunks= get_prod_monthly(chunk) ## have the preprocess chunks generator\n",
    "        for well_chunk in well_chunks: ## interate well_chunks\n",
    "            file_name = os.path.join(dir_to_save_data,'texas-wells-' + str(time.time()) +'.xlsx') ## dir to save csv file\n",
    "            wells_df.append(well_chunk) ## append result to wells_df\n",
    "            well_chunk.to_excel(file_name) ## save file to dir\n",
    "#             well_chunk.to_sql(well_dynamcis_table,con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhangchunlei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\zhangchunlei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\zhangchunlei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n",
      "yielding production data with merged api number\n",
      "yielding preprocess production data\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = 'PDQ_DSV' ## directory to holde all the csv file\n",
    "# lease_index = -1 ##lease file index\n",
    "# production_index = 8 ## production file index\n",
    "# wells_df = [] ## well df container\n",
    "# file_paths = get_file_name(base_dir) ## read in all the file path\n",
    "# lease_api_dfs = get_lease_api(file_paths,lease_index) ## generate lease api dataframe\n",
    "# retrieve_api_data = preprocess_chunk(production_index,lease_api_dfs) ## preprocess chunk to have the generator\n",
    "# for chunk in retrieve_api_data: ##iterate the chunk\n",
    "#     well_chunks= get_prod_monthly(chunk) ## have the preprocess chunks generator\n",
    "#     for well_chunk in well_chunks: ## interate well_chunks\n",
    "#         wells_df.append(well_chunk) ## append result to wells_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
